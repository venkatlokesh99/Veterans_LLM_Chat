import streamlit as st
from google import genai
from google.genai import types
from datetime import datetime
import re
from typing import Dict, List, Optional

# Page configuration
st.set_page_config(
    page_title="LLM Chat Interface",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state variables
def initialize_session_state():
    """Initialize all session state variables if they don't exist"""
    defaults = {
        'messages': [],
        'gemini_api_key': "",
        'selected_model': "Gemini 2.5 Flash",
        'temperature': 0.7,
        'top_p': 1.0,
        'use_default_temperature': False,
        'use_default_top_p': False,
        'current_response': "",
        'current_prompt': "",
        'cwe_name': "",
        'response_ready': False
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

# Model configuration
MODEL_CONFIG = {
    "Gemini 2.5 Flash": {
        "provider": "gemini",
        "model_name": "gemini-2.5-flash"
    }
}

def clear_conversation():
    """Clear the conversation history"""
    st.session_state.messages = []
    st.session_state.current_response = ""
    st.session_state.current_prompt = ""
    st.session_state.response_ready = False
    st.success("Conversation cleared!")

def format_response_for_download(response: str, prompt: str, model_name: str) -> str:
    """Format the response as clean Markdown for download"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    return f"""# AI Response

**Generated on:** {timestamp}  
**Model:** {model_name}  
**Prompt:** {prompt}

---

## Response

{response}

---

*Generated by LLM Chat Interface*
"""

def generate_filename(cwe_name: str) -> str:
    """Generate filename based on CWE name and timestamp"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if cwe_name.strip():
        clean_cwe = re.sub(r'[^\w\s-]', '', cwe_name.strip())
        clean_cwe = re.sub(r'[-\s]+', '_', clean_cwe)
        return f"Gemini_{clean_cwe}_{timestamp}.md"
    
    return f"Gemini_Response_{timestamp}.md"

def call_gemini_api(messages: List[Dict], model_name: str, api_key: str,
                   temperature: Optional[float] = None, top_p: Optional[float] = None) -> str:
    """Call Gemini API with new SDK"""
    try:
        client = genai.Client(api_key=api_key)
        
        # Convert messages to single prompt
        prompt = ""
        for msg in messages:
            role = msg["role"]
            if role == "system":
                prompt += f"System: {msg['content']}\n\n"
            elif role == "user":
                prompt += f"User: {msg['content']}\n\n"
            elif role == "assistant":
                prompt += f"Assistant: {msg['content']}\n\n"
        
        # Configure generation parameters
        config_params = {}
        if temperature is not None:
            config_params['temperature'] = temperature
        if top_p is not None:
            config_params['top_p'] = top_p
        
        config = types.GenerateContentConfig(**config_params) if config_params else types.GenerateContentConfig()
        
        response = client.models.generate_content(
            model=model_name,
            contents=prompt,
            config=config
        )
        
        return response.text
        
    except Exception as e:
        raise Exception(f"Gemini API error: {str(e)}")

def generate_response(prompt: str, model_name: str) -> str:
    """Generate response from the selected model"""
    if not st.session_state.gemini_api_key:
        raise Exception("Gemini API key is required")
    
    messages = st.session_state.messages + [{"role": "user", "content": prompt}]
    
    try:
        temperature_param = None if st.session_state.use_default_temperature else st.session_state.temperature
        top_p_param = None if st.session_state.use_default_top_p else st.session_state.top_p
        
        response = call_gemini_api(
            messages=messages,
            model_name=MODEL_CONFIG[model_name]["model_name"],
            api_key=st.session_state.gemini_api_key,
            temperature=temperature_param,
            top_p=top_p_param
        )
        
        return response
        
    except Exception as e:
        return f"Error: {str(e)}"

def main():
    """Main application function"""
    initialize_session_state()
    
    st.title("ü§ñ LLM Chat Interface")
        
    # Sidebar for configuration
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        
        # Model selection
        st.subheader("Model Selection")
        st.session_state.selected_model = st.selectbox(
            "Choose a model:",
            options=list(MODEL_CONFIG.keys()),
            index=0,
            help="Select the LLM model you want to chat with"
        )
        
        # API Key configuration
        st.subheader("API Configuration")
        st.session_state.gemini_api_key = st.text_input(
            "Gemini API Key:",
            type="password", 
            value=st.session_state.gemini_api_key,
            help="Enter your Google Gemini API key"
        )
        
        # Generation parameters
        st.subheader("Generation Parameters")
        
        # Temperature section
        st.session_state.use_default_temperature = st.checkbox(
            "Use Default", 
            value=st.session_state.use_default_temperature,
            help="Use model's default temperature instead of custom value"
        )
        
        if not st.session_state.use_default_temperature:
            st.session_state.temperature = st.slider(
                "Temperature:",
                min_value=0.0,
                max_value=2.0,
                value=st.session_state.temperature,
                step=0.1,
                help="Controls randomness in responses. Lower = more focused, Higher = more creative"
            )
        else:
            st.info("Using model's default temperature")
        
        # Top P section
        st.session_state.use_default_top_p = st.checkbox(
            "Use Default", 
            value=st.session_state.use_default_top_p,
            help="Use model's default top_p instead of custom value"
        )
        
        if not st.session_state.use_default_top_p:
            st.session_state.top_p = st.slider(
                "Top P:",
                min_value=0.0,
                max_value=1.0,
                value=st.session_state.top_p,
                step=0.05,
                help="Controls diversity via nucleus sampling"
            )
        else:
            st.info("Using model's default top_p")
        
        # Control buttons
        st.subheader("Controls")
        if st.button("üóëÔ∏è Clear Conversation", use_container_width=True):
            clear_conversation()
        
        # Download section
        st.subheader("Download")
        
        st.session_state.cwe_name = st.text_input(
            "Name of CWE:",
            value=st.session_state.cwe_name,
            placeholder="Enter CWE name (optional)",
            help="Enter the CWE name to include in the filename"
        )
        
        # Download button
        if st.session_state.response_ready and st.session_state.current_response:
            formatted_content = format_response_for_download(
                st.session_state.current_response, 
                st.session_state.current_prompt, 
                st.session_state.selected_model
            )
            filename = generate_filename(st.session_state.cwe_name)
            
            st.download_button(
                label="üì• Download Response",
                data=formatted_content,
                file_name=filename,
                mime="text/markdown",
                use_container_width=True,
                help="Download the current AI response as a Markdown file"
            )
        else:
            st.info("Generate a response to enable download")
    
    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Chat input
    if prompt := st.chat_input("Enter your message here..."):
        if not st.session_state.gemini_api_key:
            st.error("Please enter your Gemini API key in the sidebar.")
            st.stop()
        
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generate and display assistant response
        with st.chat_message("assistant"):
            with st.spinner(f"Generating response with {st.session_state.selected_model}..."):
                response = generate_response(prompt, st.session_state.selected_model)
                st.markdown(response)
        
        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": response})
        
        # Store current response for download
        st.session_state.current_response = response
        st.session_state.current_prompt = prompt
        st.session_state.response_ready = True
        
        st.rerun()

if __name__ == "__main__":
    main()